{"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2023/11/26/hello-world/"},{"title":"请求库","text":"介绍爬虫使用的请求库，requests，httpx，aiohttp。 requests 请求数据 get post 添加参数 user-agent 请求浏览器数据 date 数据 param 参数 cookie 防盗链 referer 请求超时 timeout=3 结果处理 二进制 文本text json数据 代码123456789101112131415161718192021222324252627282930313233343536import requestsurl = '这写上要爬的url'#添加头 headers = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)&quot;\\ &quot;Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.52&quot;,}#添加数据date = { &quot;kw&quot;: s}#添加参数（重新封装）params = { &quot;type&quot;: 20, &quot;interval_id&quot;: &quot;100:90&quot;, &quot;action&quot;: &quot;&quot;, &quot;start&quot;: 0, &quot;limit&quot;: 20,}#发起一个get请求resp = requests.get(url, headers=headers，data=data，params=params)#发起一个post请求resp = requests.post(url, headers=headers，data=data，params=params)#去掉安全验证resp = requests.get(url, verify=False)#设置编码格式resp.encoding = 'utf8'#状态码查看print(resp.status_code)#结果为二进制print(resp.content)#结果为json数据print(resp.json())#请求为文本print(resp.text) 中文解码12345678910import requestsfrom urllib.parse import quote,unquote#解码data='%E4%BD%A0%E5%A5%BD'print(unquote(data))#加码data2='你好'print(quote(data)) session会话 session对象 基于seesion发起的请求都会公用session内的参数。 session可以保存请求时的cookie12345678910import requestssession = requests.session()session.headers={ 'test':'test'}#请求是公用session中的参数，也可以自己添加，如果参数重合以自己的请求为主h1 =session.get('url',headers={'a':'b'})h2 =session.get('url') proxies代理123456import requestsproxies ={ 'http':'http://127.0.0.1:7890', 'https':'http://127.0.0.1:7890',}resp = requests.get('url',proxies=proxies) httpx 支持http2.0兼容http1.0 处理http1.0和requests一样的用法 12345import httpxheaders={}proies={}html = httpx.get('url',headers=headers,proies=proies)print(html.text) 处理http2.0和session用法相似 client=httpx.Client(http2.0=True,proxies=proxies) 异步请求 12345678910111213141516171819202122#第一中写法async def spider(num): print('run',num) client=httpx.AsyncClient(http2=True) html = await client.get('url') print(html) await client.aclose()#第二中写法async def spider(num): print('run',num) async with httpx.AsyncClient(http2=True) as client: html = await client.get('url') print(html)#建立队列async def main(): await asyncio.gather(*[spider(1),spider(2),spider(3)])#启动 if __name__ == '__main__': asyncio.run(main()) aiohttp 异步请求 123456789101112131415import aiohttpimport asyncioheaders={}#只支持http代理proxy='http://localhost:7890'async def main(): async with aiohttp.ClientSession(headers=headers) as client: html = await client.get('url') print(await html.text()) html.close()#使用loop启动程序loop = asyncio.get_event_loop()loop.run_until_complete(main()) 优缺点和区别 requests 同步 httpx 同步异步 aiohttp 只有异步","link":"/2023/08/01/spider1/"},{"title":"免费代理处理","text":"免费代理测试验证ip可用性 1.通过爬虫爬取免费代理ip和端口号2.对每个ip进行可用性验证，请求’https://httpbin.org/ip'判断状态码 123456789101112131415161718#封装proxyproxies = { 'http': f'http://{IP}:{PORT}', 'https': f'http://{IP}:{PORT}',}url = 'https://httpbin.org/ip' headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.78' } try: html = requests.get(url, proxies=proxies, headers=headers, timeout=3) print('状态码：', html.status_code) print('IP：', html.text) #可用的代理放入列表种 proxies_list.append(proxies) except: print('代理不可用')","link":"/2023/08/01/spider2/"},{"title":"解析库","text":"使用bs4，xpath，re进行数据解析和提取。 bs4 BeautifulSoup 支持从HTML或XML文件中提取数据的python库 Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为utf-8编码。 select方法需要使用css选择器 css选择器 class 使用·代替 id使用#代替 find方法是找到第一个满足条件的标签后就立即返回，只返回一个元素。 find_all方法是把所有满足条件的标签都选到，然后返回回去。 12345678910111213141516171819202122232425262728293031323334import requestsfrom bs4 import BeautifulSoup# 从bs对象中查找数据# find(标签，属性=值)# find_all(标签，属性-值（如果遇到python关键字用class_或者一个字典代替）)url = &quot;https://www.umei.cc/bizhitupian/diannaobizhi/&quot;resp = requests.get(url)resp.encoding = 'utf-8'#把爬下来的网站给bs并指定html解析# find(标签，属性=值)# find_all(标签，属性-值（如果遇到python关键字用class_或者一个字典代替）)main_page = BeautifulSoup(resp.text, &quot;html.parser&quot;)#main_page = BeautifulSoup(resp.text, &quot;lxml&quot;)a_list = main_page.find(&quot;div&quot;, class_=&quot;listlbc_cont_l&quot;).find_all(&quot;a&quot;)a_list = main_page.find(&quot;div&quot;, class_=&quot;item_list infinite_scroll&quot;).find_all(&quot;a&quot;)for a in a_list: href = a.get('href') print(href)#使用css选择器 调用select方法b_list = main_page.select('div.img div.btns a')#第一种写法for b in b_list: href = b.get('href') print(href)#第二种写法for b in b_list: print(b['href']) xpath //div找全部的div /div找第一个div @＋属性 严格匹配 鼠标右键复制完整xpath再元素控制台调试直到合适 12345678910111213141516import requestsfrom lxml import etreeurl = &quot;https://www.umei.cc/bizhitupian/diannaobizhi/&quot;resp = requests.get(url)resp.encoding = 'utf-8'soup = etree.HTML(resp.text)#text() @href @属性results=soup.xpath('/html/body/div[5]/div[1]/div[2]/div[1]/div/div/div[1]/div/a/img/@alt')results=soup.xpath('/html/body/div[5]/div[1]/div[2]/div[1]/div/div/div[1]/div/a/img/text()')for i in results: print(i) re re在爬虫种主要处理复杂的数据 使用正则设置一个匹配规则 得到一个list，使用group方法取出 1234567import re# （？P&lt;分组名字&gt;正则）可以单独从正则匹配的内容中进一步提取内容#re.S使.也可以匹配换行符obj = re.compile(r&quot;&lt;div class='.*?'&gt;&lt;span id='.*?'&gt;(?P&lt;分组&gt;.*?)&lt;/span&gt;&lt;div&gt;&quot;, re.S)result = obj.finditer(resp.text)for it in result: print(it.group(&quot;分组&quot;)) 总结爬虫解析页面主要使用bs4和xpath，如果遇到较为复杂的页面使用re即可。","link":"/2023/08/01/spider3/"}],"tags":[{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"爬虫","slug":"爬虫","link":"/tags/%E7%88%AC%E8%99%AB/"}],"categories":[{"name":"1.爬虫","slug":"1-爬虫","link":"/categories/1-%E7%88%AC%E8%99%AB/"},{"name":"1.爬虫基础","slug":"1-爬虫/1-爬虫基础","link":"/categories/1-%E7%88%AC%E8%99%AB/1-%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/"}],"pages":[{"title":"","text":"碎碎念 h2{ text-align: center; } p{ text-align: center; } 碎碎念 一路有陌路人陪你这一生","link":"/about/index.html"},{"title":"","text":"Title *{ margin: 0; padding: 0; font-family: \"Poppins\", sans-serif; box-sizing: border-box; } .mycontainer .mycard .face{ position: absolute; top: 0; left: 0; width: 100%; height: 100%; transition: 0.5s; } .mycontainer .mycard .face.face1{ background: transparent; display: flex; justify-content: center; align-items: center; z-index: 1; transform-origin: bottom; } .mycontainer .mycard .face.face1 img{ max-width: 150px; } .mycontainer .mycard .face.face1 h3{ margin: 10px 0 0; color: #fff; text-align: center; font-size: 1.5em; } .mycontainer .mycard .face.face2 a{ color: rgb(0, 191, 255); text-align: center; font-size: 1.5em; } 1-爬虫基础 Spiderman Lorem ipsum dolor sit amet, consectetur adipisicing elit,sed do eiusmod tempor incididunt ut labore et dolor magna aliqua. Lorem ipsum dolor sit amet, consectetur adipisicing elit,sed do eiusmod tempor incididunt ut labore et dolor magna aliqua. Lorem ipsum dolor sit amet, consectetur adipisicing elit,sed do eiusmod tempor incididunt ut labore et dolor magna aliqua.","link":"/classification/index.html"},{"title":"","text":"/* *{ margin: 0; padding: 0; font-family: \"Poppins\", sans-serif; box-sizing: border-box; } */ mybody{ height: 500px; display: flex; justify-content: center; background-color: transparents } .mycontainer{ width: 700px; height: 520px; position: relative; /* display: flex; */ justify-content: space-between; /* 开启网格布局 */ display: grid; grid-template-rows: 300px 300px ; grid-template-columns: 300px 300px ; } .mycontainer .mycard{ position: relative; width: 300px; height: 200px; } /* .mycontainer .mycard .face{ position: absolute; top: 0; left: 0; width: 100%; height: 100%; transition: 0.5s; } */ /* .mycontainer .mycard .face.face1{ background: #333; display: flex; justify-content: center; align-items: center; z-index: 1; transform-origin: bottom; } */ /* .mycontainer .mycard .face.face1 h3{ margin: 10px 0 0; color: #fff; text-align: center; font-size: 1.5em; } */ .mycontainer .mycard:hover .face.face1{ transform: translateY(-100%) rotateX(90deg); background: #07a5fb; } .mycontainer .mycard .face.face2{ background: #fff; display: flex; justify-content: center; align-items: center; padding: 20px; transform-origin: top; transform: translateY(100%) rotateX(90deg); } .mycontainer .mycard:hover .face.face2{ transform: translateY(0%) rotateX(0deg); }","link":"/classification/style.css"},{"title":"","text":"","link":"/codebase/index.html"}]}